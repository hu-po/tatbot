# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Tatbot is a tattoo robot system that uses dual Trossen WidowXAI arms, Intel RealSense D405 depth cameras, Amcrest IP cameras, and AI models for autonomous tattooing. The system is distributed across multiple nodes with MCP (Model Context Protocol) servers for remote control and coordination.

## Common Development Commands

### Environment Setup
```bash
# Initial setup with uv
source scripts/setup_env.sh
uv pip install -e .

# Install node-specific dependencies
uv pip install .[bot,cam,dev,gen,gpu,img,viz]

# Load environment variables (API keys, passwords)
set -a; source .env; set +a
```

### Code Quality
```bash
# Run linting and formatting
./scripts/lint.sh

# Or manually:
uv run isort .
uv run ruff check --config pyproject.toml --fix
```

### MCP Server Operations
```bash
# Start MCP server for a specific node
./scripts/run_mcp.sh <node_name>  # ook, oop, ojo, trossen-ai, rpi1, rpi2

# Kill existing MCP processes
./scripts/kill.sh

# Monitor MCP logs
tail -f ~/tatbot/nfs/mcp-logs/<node_name>.log
```

### Running Operations
```bash
# Run main application with Hydra configuration
uv run python -m tatbot.main

# Run with specific scene
uv run python -m tatbot.main scenes=tatbotlogo

# Run visualization tools
uv run python -m tatbot.viz.stroke --scene=tatbotlogo
uv run python -m tatbot.viz.teleop --scene=default
uv run python -m tatbot.viz.map --scene=default
```

## Architecture

### Core Components

1. **MCP Servers** (`src/tatbot/mcp/`)
   - Distributed control system using Model Context Protocol
   - Each node runs an MCP server with specific capabilities
   - Tools are dynamically registered based on node configuration
   - Uses Hydra for configuration management

2. **Configuration System** (`src/conf/`)
   - Hydra-based configuration with defaults and overrides
   - Scene configurations combine arms, cameras, inks, poses
   - All configs validate through Pydantic schemas

3. **Robot Control** (`src/tatbot/bot/`)
   - Trossen arm control via `trossen_arm` library
   - URDF-based kinematics
   - Dual-arm coordination (left/right)

4. **Vision System** (`src/tatbot/cam/`)
   - Intel RealSense depth cameras
   - AprilTag tracking for localization
   - Camera calibration and extrinsics

5. **Generation Pipeline** (`src/tatbot/gen/`)
   - G-code parsing and stroke generation (`gcode.py`)
   - Inverse kinematics solving using JAX (`ik.py`)
   - 2D to 3D surface mapping using geodesic paths (`map.py`)
   - Stroke batch processing for GPU acceleration (`batch.py`)
   - Ink dipping trajectory generation (`inkdip.py`)

6. **Operations** (`src/tatbot/ops/`)
   - Recording and playback workflows
   - Stroke execution
   - System reset procedures

### Node Topology

- **ook** (ðŸ¦§): Acer Nitro V 15 with NVIDIA RTX 4050, primary compute for VLA models
- **oop** (ðŸ¦Š): Ubuntu PC with NVIDIA RTX 3090 (home mode only)
- **ojo** (ðŸ¦Ž): NVIDIA Jetson AGX Orin, runs agent models via Ollama
- **trossen-ai** (ðŸ¦¾): System76 Meerkat PC, robot arm control and RealSense cameras
- **rpi1/rpi2** (ðŸ“ðŸ‡): Raspberry Pi 5 nodes, rpi2 serves as NFS server
- **camera1-5** (ðŸ“·): Amcrest IP PoE cameras for scene coverage
- **realsense1-2** (ðŸ“·): Intel RealSense D405 depth cameras mounted on arms

### Data Flow

1. **Design Generation**: Image â†’ DrawingBotV3 â†’ G-code files
2. **Stroke Processing**: G-code â†’ `make_gcode_strokes` â†’ `StrokeList`
3. **Surface Mapping**: `StrokeList` â†’ `map_strokes_to_mesh` â†’ 3D mapped strokes
4. **IK Solving**: Mapped strokes â†’ `batch_ik` â†’ `StrokeBatch` with joint angles
5. **Execution**: `StrokeBatch` â†’ `StrokeOp` â†’ Robot arms via LeRobot interface

## Key Design Patterns

- **Hydra Configuration**: All components configured via YAML with schema validation
- **MCP Distribution**: Tools exposed as MCP endpoints for remote execution
- **Pydantic Validation**: Strong typing and validation throughout
- **Virtual Environment**: Uses `uv` for fast, deterministic dependency management
- **Modular Extras**: Optional dependencies grouped by functionality
- **Configuration Constants**: Centralized constant classes replace magic numbers (AppConstants, ServerConstants, MCPConstants, CalibrationConstants)
- **Specific Exception Handling**: Custom exception types (ConfigurationError, NetworkError, CalibrationError, etc.) replace generic Exception catching
- **Descriptive Naming**: Variables use meaningful names instead of cryptic abbreviations

## Critical Files

- `src/tatbot/main.py`: Entry point with Hydra initialization (enhanced with AppConstants and improved exception handling)
- `src/tatbot/mcp/server.py`: MCP server implementation (enhanced with ServerConstants and type hints)
- `src/tatbot/mcp/handlers.py`: MCP tool implementations (enhanced with specific exception handling)
- `src/tatbot/mcp/models.py`: MCP request/response models (enhanced with MCPConstants)
- `src/tatbot/config_schema.py`: Pydantic schemas for configuration (enhanced with proper type annotations)
- `src/tatbot/bot/trossen_homing.py`: Arm calibration (enhanced with CalibrationConstants)
- `src/tatbot/gen/batch.py`: Stroke batch processing (enhanced with descriptive variable names)
- `src/conf/config.yaml`: Root Hydra configuration
- `pyproject.toml`: Project dependencies and metadata

## Important Notes

- Always use `uv` for Python package management
- MCP server changes require manual restart in Cursor UI (Ctrl+Shift+P > "View: OpenMCP Settings")
- System uses Python 3.11.10 (strict requirement)
- Distributed system - ensure network connectivity between nodes
- Camera passwords and API keys stored in `.env` file
- NFS mount required: `~/tatbot/nfs` shared across all nodes (served by rpi2)
- Designs stored in `tatbot/nfs/designs/` directory
- DrawingBotV3 configs in `config/dbv3/` for pen settings and G-code generation
- Both RealSense cameras connected to trossen-ai via USB3